# Token Tracking System for Agent Orchestra

## Overview

This document describes the token tracking system for monitoring resource usage across all agents in the hierarchical architecture.

## Architecture

The token tracking system monitors three key metrics:
1. **Input tokens** - Tokens sent to agents (prompt size)
2. **Output tokens** - Tokens generated by agents (response size)
3. **Cost** - Dollar cost based on model pricing

## Tracking Points

### Per-Agent Tracking

Each agent invocation logs:
- Agent ID/name
- Timestamp
- Model used (Opus/Sonnet/Haiku)
- Input tokens
- Output tokens
- Task ID
- Duration
- Success/failure status
- Escalation level (if applicable)

### Per-Tier Aggregation

Aggregate metrics at each tier:
- Strategic (5 agents) - Monitor high-value invocations
- Tactical (10 coordinators) - Monitor coordination overhead
- Execution (32 specialists) - Monitor specialist efficiency

### Per-Department Aggregation

Aggregate metrics per department:
- Frontend department (4 specialists)
- Backend department (8 specialists)
- Testing department (3 specialists)
- DevOps department (4 specialists)
- Data department (7 specialists)
- Mobile department (3 specialists)
- Documentation department (3 specialists)
- Integration department (3 specialists)
- Security department (4 specialists)
- Quality department (4 specialists)

## Storage Format

Metrics are stored in `.claude/metrics/` directory as JSON files:

```
.claude/metrics/
├── agents/
│   ├── tech-lead-orchestrator.json
│   ├── react-expert.json
│   ├── frontend-coordinator.json
│   └── ... (one file per agent)
├── departments/
│   ├── frontend-department.json
│   ├── backend-department.json
│   └── ... (one file per department)
├── tiers/
│   ├── strategic-tier.json
│   ├── tactical-tier.json
│   └── execution-tier.json
└── system.json (overall system metrics)
```

## Data Format (Per-Agent JSON)

```json
{
  "agent_id": "react-expert",
  "agent_name": "React Expert",
  "model": "sonnet",
  "tier": "execution",
  "department": "frontend",
  "metrics": {
    "summary": {
      "total_invocations": 42,
      "successful_tasks": 40,
      "failed_tasks": 2,
      "success_rate": 0.952,
      "total_tokens_input": 125000,
      "total_tokens_output": 45000,
      "total_tokens": 170000,
      "avg_tokens_per_task": 4047,
      "total_cost": 0.67,
      "avg_cost_per_task": 0.016
    },
    "performance": {
      "avg_completion_time_seconds": 145.2,
      "p95_completion_time_seconds": 320.5,
      "min_completion_time_seconds": 12.3,
      "max_completion_time_seconds": 450.0
    },
    "quality": {
      "self_recovery_count": 5,
      "peer_consultation_count": 2,
      "coordinator_escalation_count": 1,
      "human_escalation_count": 0,
      "test_pass_rate": 0.975,
      "code_review_rejections": 1,
      "avg_quality_score": 0.93
    },
    "budget": {
      "budget_warnings": 0,
      "budget_overruns": 0,
      "max_tokens_in_session": 32000,
      "current_session_tokens": 8500
    },
    "timeline": {
      "first_invocation": "2025-11-04T10:00:00Z",
      "last_invocation": "2025-11-04T16:30:00Z",
      "session_duration_minutes": 390
    },
    "recent_tasks": [
      {
        "task_id": "T042",
        "timestamp": "2025-11-04T16:30:00Z",
        "status": "success",
        "tokens_input": 3200,
        "tokens_output": 1100,
        "cost": 0.0152,
        "duration_seconds": 82.5,
        "escalation_level": 0
      }
    ]
  }
}
```

## Cost Calculation

Pricing per model (as of November 2024):

```
Opus:
  - Input: $15 per 1M tokens
  - Output: $75 per 1M tokens

Sonnet:
  - Input: $3 per 1M tokens
  - Output: $15 per 1M tokens

Haiku:
  - Input: $0.80 per 1M tokens
  - Output: $4 per 1M tokens
```

Cost formula:
```
cost = (input_tokens * input_price / 1_000_000) + (output_tokens * output_price / 1_000_000)
```

## Monitoring Dashboards

### Agent-Level Dashboard

Shows per-agent metrics:
- Success rate
- Average completion time
- Cost per task
- Token efficiency trend
- Escalation frequency

### Department-Level Dashboard

Aggregates all specialists in a department:
- Department success rate
- Average completion time
- Department cost
- Specialist utilization
- Quality gate pass rate

### System-Level Dashboard

Shows overall system health:
- System success rate
- Average mission time
- Total weekly cost
- Communication overhead
- Tier utilization breakdown
- Hallucination incidents
- Error recovery effectiveness

### Cost Trend Dashboard

Tracks cost over time:
- Daily/weekly/monthly spend
- Cost per task trend (should decrease)
- Model distribution (Opus/Sonnet/Haiku usage)
- Budget overruns and warnings
- Cost optimization recommendations

## Budget Management

### Token Budgets

Per-tier budgets:

```
Strategic Tier:
  Tmax (input limit): 128,000 tokens
  Tretained (output limit): 16,000 tokens
  Budget warning: 80% = 102,400 tokens
  Budget hard limit: 100% = 128,000 tokens

Tactical Tier:
  Tmax: 64,000 tokens
  Tretained: 8,000 tokens
  Budget warning: 80% = 51,200 tokens
  Budget hard limit: 100% = 64,000 tokens

Execution Tier:
  Tmax: 32,000 tokens
  Tretained: 4,000 tokens
  Budget warning: 80% = 25,600 tokens
  Budget hard limit: 100% = 32,000 tokens
```

### Budget Enforcement

When an agent approaches budget:

**80% (Warning Level):**
- Log warning: "Agent {name} approaching context limit"
- Notify coordinator
- Track for future optimization

**100% (Hard Limit):**
- Force context compression
- Trigger summarization
- Escalate to context-manager
- May delay execution pending compression

## Optimization Triggers

Automatic triggers that identify optimization opportunities:

### Trigger 1: High Token Cost Agent
- **Condition:** Agent cost > 120% of department average
- **Action:** Flag for prompt optimization, consider MIPRO analysis
- **Example:** "react-expert using 2x more tokens than nextjs-specialist"

### Trigger 2: Low Success Rate Agent
- **Condition:** Success rate < 85%
- **Action:** Analyze error patterns, provide guidance, may need model upgrade
- **Example:** "haiku-based agent needs Sonnet upgrade"

### Trigger 3: High Escalation Rate
- **Condition:** Escalation rate > 15%
- **Action:** Prompt needs improvement or model upgrade needed
- **Example:** "Mobile coordinator escalating 20% of tasks"

### Trigger 4: Department Overspend
- **Condition:** Department cost > 110% of budget
- **Action:** Analyze specialists, optimize highest-cost agents
- **Example:** "Frontend department over budget, optimize react-expert"

### Trigger 5: System Overspend
- **Condition:** Total system cost > 110% of monthly budget
- **Action:** Comprehensive review, prioritize high-impact optimizations
- **Example:** "Monthly cost $400, budget is $300, need optimization"

### Trigger 6: Long-Running Task
- **Condition:** Task exceeds Tretained without completion
- **Action:** Force incremental summarization, may need to split task
- **Example:** "Large refactoring task approaching memory limit"

## Implementation Checklist

- [ ] Create `.claude/metrics/` directory structure
- [ ] Implement token tracking for all agent invocations
- [ ] Create agent-level metric aggregation
- [ ] Create department-level metric aggregation
- [ ] Create tier-level metric aggregation
- [ ] Create system-level metric aggregation
- [ ] Implement cost calculation
- [ ] Create real-time cost dashboard
- [ ] Implement automated budget enforcement
- [ ] Implement optimization trigger detection
- [ ] Create weekly metrics report
- [ ] Create monthly cost analysis report
- [ ] Set up alerts for budget overruns

## Usage Example

When invoking an agent via Task tool:

```
Before Task:
- Record timestamp
- Note budget remaining

After Task:
- Record input tokens (from model response)
- Record output tokens (from model response)
- Calculate cost
- Update agent metrics
- Check for budget warnings
- Check for optimization triggers
- Log to `.claude/metrics/[agent-id].json`
```

## Metrics to Optimize (Phase 1)

1. **Token Efficiency**
   - Baseline: Measure current tokens per task
   - Target: 30% reduction via context filtering
   - Timeline: Phase 2-3

2. **Cost Efficiency**
   - Baseline: Current cost (estimated $1,058/month with all Sonnet)
   - Target: 72% reduction (estimated $298/month)
   - Timeline: Phase 4 (after model reassignment)

3. **Success Rate**
   - Baseline: Current success rate (estimate 85%)
   - Target: ≥95%
   - Timeline: Continuous

4. **Completion Time**
   - Baseline: Current average (measure in Phase 1)
   - Target: ≤ baseline (hierarchical should not slow things down)
   - Timeline: Continuous

## Integration with CLAUDE.md

The token tracking system integrates with:

1. **Budget enforcement** - Hard limits in CLAUDE.md Phase 2
2. **Error escalation** - Token budget overrun triggers Level 3 escalation
3. **Approval gates** - Cost visible in execution plans for user awareness
4. **Optimization process** - Triggers feed into Phase 5 optimization loop

## Future Enhancements

- **Machine learning** - Predict token usage before invocation
- **Auto-compression** - Automatically compress context when approaching limits
- **Model optimization** - Suggest Opus→Sonnet→Haiku downgrades based on metrics
- **Caching** - Cache similar requests to reduce token usage
- **Batching** - Batch similar agent tasks to amortize overhead

---

This system provides full visibility into resource usage and enables data-driven optimization of the Agent Orchestra.
